# Stream A: Naive RAG Baseline Results

**Date:** 2026-02-07
**Status:** Partial (80/100 questions completed - process interrupted)

## Summary

| Metric | Value |
|--------|-------|
| **Overall Accuracy** | **81.9%** |
| Questions Evaluated | 80/100 |
| Correct (1.0) | 64 |
| Partial (0.5) | 3 |
| Wrong (0.0) | 13 |
| Total Score | 65.5 |

## Per Question Type Breakdown

| Category | Accuracy | Score/Total |
|----------|----------|-------------|
| single-session-user | 87.1% | 61.0/70 |
| multi-session | 45.0% | 4.5/10 |

## Comparison to SOTA

| System | Accuracy | Notes |
|--------|----------|-------|
| **Naive RAG (PIE)** | **81.9%** | This run (incomplete) |
| Emergence | 86% | Published SOTA |
| Zep | 71.2% | Published baseline |

### Key Observations:

1. **Strong single-session performance:** 87.1% on single-session questions shows naive RAG works well when all context is retrievable from one session.

2. **Multi-session weakness:** Only 45% on multi-session questions - this is expected since naive RAG doesn't aggregate information across sessions.

3. **Competitive with SOTA:** 81.9% puts us between Zep (71.2%) and Emergence (86%). With temporal/graph enhancements, we should beat Emergence.

4. **_abs questions are hard:** The `_abs` variant questions (absolute references like "my hamster" â†’ "my cat") caused several failures - these require entity disambiguation.

## Technical Details

- **Model:** gpt-4o
- **Retrieval:** 10 chunks per query
- **Embedding:** text-embedding-3-small (assumed)
- **Average latency:** ~5 seconds/question

## Next Steps

1. Complete remaining 20 questions when API is stable
2. Compare with `pie_temporal` baseline for temporal reasoning questions
3. Analyze failure modes for multi-session questions
4. Test graph-aware retrieval for entity disambiguation

---
*Generated by Stream A subagent*
